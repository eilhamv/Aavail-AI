{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Capstone Project - Model Building and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Engineering\n",
    "\n",
    "* **Engineer Features with Rolling Window**: When forecasting the revenues only the realized (historical) values are known. The unrealized values are not given. Therefore for each instance in the dataset I have to come up with additional features that refer to historical values.\n",
    "\n",
    "\n",
    "* **Target Value Rolling Window**: To standardize the approach across models ensure the target is the sum of revenues over the next 30 days for any given point in time. For supervised learning, the total revenue over the next thirty days can be represented as a single number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_engineering.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## import function to load feature matrix\n",
    "from data_ingestion import load_feature_matrix\n",
    "from data_ingestion import DEV\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    apply rolling window to engineer features\n",
    "    \"\"\"\n",
    "    def __init__(self, shift=1, attributes=['revenue'], func=\"sum\"):\n",
    "        self.shift = shift\n",
    "        self.attributes = attributes\n",
    "        self.freq = \"{}D\".format(self.shift)\n",
    "        self.func = func\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        ## set time as index\n",
    "        X_ts = X.set_index([\"invoice_date\"])\n",
    "        \n",
    "        ## rolling window\n",
    "        if self.func == \"sum\":\n",
    "            X_eng = X_ts[self.attributes].rolling(self.freq, closed=\"left\").sum()\n",
    "        else:\n",
    "            X_eng = X_ts[self.attributes].rolling(self.freq, closed=\"left\").mean()\n",
    "        \n",
    "        ## merge with initial dataset and fill NAs\n",
    "        X_eng = X_ts.merge(X_eng, left_index=True, right_index=True, how=\"left\", suffixes=[\"\",\"_m{}\".format(self.freq)]).fillna(0)\n",
    "        return X_eng.reset_index()\n",
    "\n",
    "class TargetEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    apply day rolling window and shift backwards\n",
    "    to engineer the predicted summed revenue\n",
    "    \"\"\"\n",
    "    def __init__(self, shift=30, attributes=['revenue']):\n",
    "        self.shift = shift\n",
    "        self.attributes = attributes\n",
    "        self.freq = \"{}D\".format(self.shift)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        ## set time as index\n",
    "        X_ts = X.set_index([\"invoice_date\"])\n",
    "        \n",
    "        ## rolling window\n",
    "        X_eng = X_ts[self.attributes].rolling(self.freq, closed=\"left\").sum().shift(-self.shift,\"D\")\n",
    "        \n",
    "        ## merge with the original\n",
    "        X_eng = X_ts.merge(X_eng, left_index=True, right_index=True, how=\"left\", suffixes=[\"\",\"_p{}\".format(self.freq)])\n",
    "        \n",
    "        return X_eng.reset_index()\n",
    "\n",
    "def engineer_features(training=False, clean=False, dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    engineer feature matrix and target value\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load feature matrix\n",
    "    fm = load_feature_matrix(dev=dev, clean=clean, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Engineering Features and Target\")\n",
    "    \n",
    "    ## ensure that all days are accounted for each country\n",
    "    fm = fm.set_index([\"invoice_date\",\"country\"]).unstack(1).asfreq(\"1D\")\n",
    "   \n",
    "    ## fill NAs with zero (assume no revenue was generated that day)\n",
    "    fm = fm.fillna(0).stack(1).reset_index()\n",
    "    \n",
    "    ## unique countries\n",
    "    countries = fm.country.unique()\n",
    "    \n",
    "    ## original features\n",
    "    features = [\"invoice_date\",\"purchases\",\"unique_invoices\",\"unique_streams\",\"total_views\",\"revenue\"]\n",
    "    \n",
    "    ## non revenue feautures\n",
    "    nonrevenue_features = [\"purchases\",\"unique_invoices\",\"unique_streams\",\"total_views\"]\n",
    "    \n",
    "    eng_features = {}\n",
    "    for country in countries:\n",
    "        \n",
    "        ## filter on country\n",
    "        df = fm.query(\"country==@country\").drop(\"country\", axis=1)\n",
    "        \n",
    "        ## build pipeline to transform features\n",
    "        pipe_engineer = Pipeline([\n",
    "            (\"revenue7d\", FeatureEngineer(shift=7)),\n",
    "            (\"revenue14D\", FeatureEngineer(shift=14)),\n",
    "            (\"revenue28D\", FeatureEngineer(shift=28)),\n",
    "            (\"revenue35D\", FeatureEngineer(shift=35)),\n",
    "            (\"revenue54D\", FeatureEngineer(shift=54)),\n",
    "            (\"nonrevenue30D\", FeatureEngineer(attributes=nonrevenue_features, shift=30, func=\"mean\"))\n",
    "        ])\n",
    "        \n",
    "        ## engineer features\n",
    "        X = pipe_engineer.transform(df)\n",
    "        \n",
    "        ## keep the dates\n",
    "        dates = X[[\"invoice_date\"]].copy()\n",
    "        \n",
    "        ## features sanity ckeck\n",
    "        test_date = dates.invoice_date.dt.strftime('%Y-%m-%d')[10]\n",
    "        v1 = df.query(\"invoice_date<@test_date\")[[\"revenue\"]].tail(14).sum().values\n",
    "        v2 = X.query(\"invoice_date==@test_date\")[\"revenue_m14D\"].values.ravel()\n",
    "        if not np.array_equal(v1.round(2), v2.round(2)):\n",
    "            print(\"Opps! Engineer features didn't work as expected\")\n",
    "            \n",
    "        ## drop original features\n",
    "        X.drop(features, axis=1, inplace=True)\n",
    "        \n",
    "        ## keep the names of the engineered features\n",
    "        features_labels = X.columns.values\n",
    "        \n",
    "        ## build instance to transform target\n",
    "        target_eng = TargetEngineer()\n",
    "        \n",
    "        ## engineer target\n",
    "        y = target_eng.transform(df)\n",
    "        \n",
    "        ## target sanity check\n",
    "        v1 = df.query(\"invoice_date>=@test_date\")[[\"revenue\"]].head(30).sum().values\n",
    "        v2 = y.query(\"invoice_date>=@test_date\").head(1)[\"revenue_p30D\"].values.ravel()\n",
    "        if not np.array_equal(v1.round(2), v2.round(2)):\n",
    "            print(\"Opps! Engineer target didn't work as expected\")\n",
    "        \n",
    "        ## drop original features\n",
    "        y.drop(features, axis=1, inplace=True)\n",
    "            \n",
    "        if training:\n",
    "            ## remove dates with NAs\n",
    "            ## the 30-day rolling and shift back for the target results in NAs for the last 30 days\n",
    "            mask = y[\"revenue_p30D\"].notna()\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "            dates = dates[mask]\n",
    "            X.reset_index(drop=True, inplace=True)\n",
    "            y.reset_index(drop=True, inplace=True)\n",
    "            dates.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        ## store them as numpy arrays\n",
    "        eng_features[country]= (X.values, y.values.ravel(), dates.values.ravel(), features_labels)\n",
    "    \n",
    "    return eng_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    run_start = time.time()\n",
    "  \n",
    "    ## engineer data\n",
    "    datasets = engineer_features(training=True, dev=DEV)\n",
    "    \n",
    "    print(\"METADATA\")\n",
    "    for key, item in datasets.items():\n",
    "        print(\"...{} X:{}, y:{}\".format(key.upper(), item[0].shape, item[1].shape))\n",
    "    \n",
    "    ## metadata\n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"...run time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "    \n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting Data\n",
      "...loading timeseries data from files\n",
      "Creating Feature Matrix\n",
      "Engineering Features and Target\n",
      "METADATA\n",
      "...SINGAPORE X:(581, 9), y:(581,)\n",
      "...NETHERLANDS X:(581, 9), y:(581,)\n",
      "...HONG_KONG X:(581, 9), y:(581,)\n",
      "...NORWAY X:(581, 9), y:(581,)\n",
      "...PORTUGAL X:(581, 9), y:(581,)\n",
      "...EIRE X:(581, 9), y:(581,)\n",
      "...TOTAL X:(581, 9), y:(581,)\n",
      "...GERMANY X:(581, 9), y:(581,)\n",
      "...SPAIN X:(581, 9), y:(581,)\n",
      "...FRANCE X:(581, 9), y:(581,)\n",
      "...UNITED_KINGDOM X:(581, 9), y:(581,)\n",
      "...run time: 0:00:00\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%run data_engineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/eilhamv/.local/lib/python3.5/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/eilhamv/.local/lib/python3.5/site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/eilhamv/.local/lib/python3.5/site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/eilhamv/.local/lib/python3.5/site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/eilhamv/.local/lib/python3.5/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Selection\n",
    "\n",
    "For each country a model is built and trained. Since the features have different scales, we need to scale the data. The algorithms explored are:\n",
    "\n",
    "* Stochastic Gradient Descent\n",
    "* Random Forest Regressor\n",
    "* Gradient Bossting Regressor\n",
    "* Ada Boosting Regressor\n",
    "\n",
    "For each algorithm scikit-learn `Pipeline` and `GridSearchCV` is used to fine tune the hyperparameters and find the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logger.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logger.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import time,os,re,csv,sys,uuid,joblib\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "PROJECT_DIR = \".\"\n",
    "LOG_DIR = os.path.join(\"logs\")\n",
    "\n",
    "\n",
    "## import mode\n",
    "from data_ingestion import DEV\n",
    "\n",
    "def _update_train_log(tag,algorithm,score,runtime,model_version,model_note,dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    update train log file\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"...updating train log\")\n",
    "    \n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.makedirs(LOG_DIR)\n",
    "        \n",
    "    ## name the logfile using something that cycles with date (day, month, year)    \n",
    "    today = date.today()\n",
    "    if dev:\n",
    "        logfile = \"{}-train-{}-{}.log\".format(\"test\",today.year, today.month)\n",
    "    else:\n",
    "        logfile = \"{}-train-{}-{}.log\".format(\"prod\",today.year, today.month)\n",
    "        \n",
    "    \n",
    "    ## write the data to a csv file\n",
    "    logpath = os.path.join(LOG_DIR, logfile)\n",
    "    \n",
    "    ## write the data to a csv file    \n",
    "    header = [\"unique_id\",\"timestamp\",'tag','score',\"runtime\",'model_version','model_note']\n",
    "    write_header = False\n",
    "    if not os.path.exists(logpath):\n",
    "        write_header = True\n",
    "    with open(logpath,'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|')\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        to_write = map(str,[uuid.uuid4(),time.time(),tag,algorithm,score,runtime,model_version,model_note])\n",
    "        writer.writerow(to_write)\n",
    "        \n",
    "def _update_predict_log(tag,y_pred,target_date,runtime,model_version,model_note,dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    update predict log file\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"...update predict log\")\n",
    "    \n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.makedirs(LOG_DIR)\n",
    "        \n",
    "    ## name the logfile using something that cycles with date (day, month, year)    \n",
    "    today = date.today()\n",
    "    if dev:\n",
    "        logfile = \"{}-predict-{}-{}.log\".format(\"test\",today.year, today.month)\n",
    "    else:\n",
    "        logfile = \"{}-predict-{}-{}.log\".format(\"prod\",today.year, today.month)\n",
    "        \n",
    "    \n",
    "    ## write the data to a csv file\n",
    "    logpath = os.path.join(LOG_DIR, logfile)\n",
    "    \n",
    "    ## write the data to a csv file    \n",
    "    header = [\"unique_id\",\"timestamp\",'tag','y_pred',\"target_date\",\"runtime\",'model_version','model_note']\n",
    "    write_header = False\n",
    "    if not os.path.exists(logpath):\n",
    "        write_header = True\n",
    "    with open(logpath,'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|')\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        to_write = map(str,[uuid.uuid4(),time.time(),tag,y_pred,target_date,runtime,model_version,model_note])\n",
    "        writer.writerow(to_write)\n",
    "        \n",
    "def log_load(tag,year,month,env,verbose=True):\n",
    "    \"\"\"\n",
    "    load requested log file\n",
    "    \"\"\"\n",
    "    logfile = \"{}-{}-{}-{}.log\".format(env,tag,year,month)\n",
    "    \n",
    "    if verbose:\n",
    "        print(logfile)\n",
    "    return logfile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modelling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modelling.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import time,os,re,csv,sys,uuid,joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "MODEL_DIR = os.path.join(\"models\")\n",
    "MODEL_VERSION = 0.1\n",
    "MODEL_VERSION_NOTE = \"supervised learing model for time-series\"\n",
    "\n",
    "## load from project package\n",
    "from data_ingestion import DEV\n",
    "from data_engineering import engineer_features\n",
    "from data_visualization import save_fig\n",
    "from logger import _update_train_log, _update_predict_log\n",
    "\n",
    "def _plot_learning_curve(estimator, X, y, ax=None, cv=5):\n",
    "    \"\"\"\n",
    "    an sklearn estimator \n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    train_sizes=np.linspace(.1, 1.0, 6)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y,\n",
    "                                                            cv=cv, n_jobs=-1,\n",
    "                                                            train_sizes=train_sizes,scoring=\"neg_mean_squared_error\")\n",
    "    train_scores_mean = np.mean(np.sqrt(-train_scores), axis=1)\n",
    "    train_scores_std = np.std(np.sqrt(-train_scores), axis=1)\n",
    "    test_scores_mean = np.mean(np.sqrt(-test_scores), axis=1)\n",
    "    test_scores_std = np.std(np.sqrt(-test_scores), axis=1)\n",
    "\n",
    "    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    ## axes and lables\n",
    "    buff = 0.05\n",
    "    xmin,xmax = ax.get_xlim()\n",
    "    ymin,ymax = ax.get_ylim()\n",
    "    xbuff = buff * (xmax - xmin)\n",
    "    ybuff = buff * (ymax - ymin)\n",
    "    ax.set_xlim(xmin-xbuff,xmax+xbuff)\n",
    "    ax.set_ylim(ymin-ybuff,ymax+ybuff)\n",
    "    ax.set_xlabel(\"Training examples\")\n",
    "    ax.set_ylabel(\"MSE Score\")\n",
    "    \n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def _make_compare_plot(X, y, models, verbose=True):\n",
    "    \"\"\"\n",
    "    create learning curves for SGD, RF, GB and ADA\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8), facecolor=\"white\")\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"...creating learning curves\")\n",
    "    \n",
    "    ## SGD\n",
    "    reg1 = SGDRegressor(**models[\"SGD\"][1])\n",
    "    pipe1 = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                            (\"reg\", reg1)])\n",
    "    _plot_learning_curve(pipe1, X, y, ax=ax1)\n",
    "    ax1.set_title(\"Stochastic Gradient Regressor\")\n",
    "    ax1.set_xlabel(\"\")\n",
    "    \n",
    "    ## random forest\n",
    "    reg2 = RandomForestRegressor(**models[\"RF\"][1])\n",
    "    pipe2 = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                            (\"reg\", reg2)])\n",
    "    _plot_learning_curve(pipe2, X, y, ax=ax2)\n",
    "    ax2.set_title(\"Random Forest Regressor\")\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.set_ylabel(\"\")\n",
    "    \n",
    "    ## gradient boosting\n",
    "    reg3 = GradientBoostingRegressor(**models[\"GB\"][1])\n",
    "    pipe3 = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                            (\"reg\", reg3)])\n",
    "    _plot_learning_curve(pipe3, X, y, ax=ax3)\n",
    "    ax3.set_title(\"Gradient Boosting Regressor\")\n",
    "    \n",
    "    ## ada boosting\n",
    "    reg4 = AdaBoostRegressor(**models[\"ADA\"][1])\n",
    "    pipe4 = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                            (\"reg\", reg4)])\n",
    "    _plot_learning_curve(pipe3, X, y, ax=ax4)\n",
    "    ax4.set_title(\"Ada Boosting Regressor\")\n",
    "    ax4.set_ylabel(\"\")\n",
    "    \n",
    "    ymin, ymax = [], []\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        \n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.set_ylim([ymin.min(), ymax.max()])\n",
    "        \n",
    "        \n",
    "def _plot_feature_importance(estimator, feature_names, verbose=True):\n",
    "    \"\"\"\n",
    "    plot feature importance\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"...plotting feature importance\")\n",
    "        \n",
    "    fig = plt.figure(figsize=(8, 6), facecolor=\"white\")\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # make importances relative to max importance\n",
    "    feature_importance = estimator.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    \n",
    "    ax.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, feature_names[sorted_idx])\n",
    "    ax.set_xlabel('Relative Importance')\n",
    "    ax.set_title('Variable Importance')\n",
    "    \n",
    "\n",
    "def _model_train(X, y, feature_names, tag=\"total\", rs=42, save_img=False, dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    train four models (SGD, RF, GB, ADA) and select the best one\n",
    "    \"\"\"\n",
    "    \n",
    "    ## start timer for runtime\n",
    "    time_start = time.time()\n",
    "    \n",
    "    ## split the dataset into train and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=rs)\n",
    "    \n",
    "    ## build models\n",
    "    regressor_names = [\"SGD\",\"RF\", \"GB\", \"ADA\"]\n",
    "    \n",
    "    regressors = (SGDRegressor(random_state=rs),\n",
    "                  RandomForestRegressor(random_state=rs),\n",
    "                  GradientBoostingRegressor(random_state=rs),\n",
    "                  AdaBoostRegressor(random_state=rs))\n",
    "    \n",
    "    params = [\n",
    "        {\"reg__penalty\":[\"l1\",\"l2\",\"elasticnet\"],\n",
    "         \"reg__learning_rate\":[\"constant\",\"optimal\",\"invscaling\"]},\n",
    "        {\"reg__n_estimators\":[10,30,50],\n",
    "         \"reg__max_features\":[3,4,5],\n",
    "         \"reg__bootstrap\":[True, False]},\n",
    "        {\"reg__n_estimators\":[10,30,50],\n",
    "         \"reg__max_features\":[3,4,5],\n",
    "         \"reg__learning_rate\":[1, 0.1, 0.01, 0.001]},\n",
    "        {\"reg__n_estimators\":[10,30,50],\n",
    "         \"reg__learning_rate\":[1, 0.1, 0.01, 0.001]}]\n",
    "    \n",
    "    \n",
    "    ## train models\n",
    "    models = {}\n",
    "    total = len(regressor_names)\n",
    "    for iteration, (name,regressor,param) in enumerate(zip(regressor_names, regressors, params)):\n",
    "        \n",
    "        if verbose:\n",
    "            end = \"\" if (iteration+1) < total else \"\\n\"\n",
    "            print(\"\\r...training model: {}/{}\".format(iteration+1,total), end=end)\n",
    "        \n",
    "        pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                               (\"reg\", regressor)])\n",
    "        \n",
    "        grid = GridSearchCV(pipe, param_grid=param, \n",
    "                            scoring=\"neg_mean_squared_error\",\n",
    "                            cv=5, n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        models[name] = grid, grid.best_estimator_[\"reg\"].get_params()\n",
    "        \n",
    "    ## plot learning curves\n",
    "    if save_img:\n",
    "        _make_compare_plot(X, y, models=models, verbose=verbose)\n",
    "        save_fig(\"{}_learning_curves\".format(tag))\n",
    "    \n",
    "    ## evaluation on the validation set\n",
    "    val_scores = []\n",
    "    for key, model in models.items():\n",
    "        y_pred = model[0].predict(X_valid)\n",
    "        rmse = np.sqrt(mean_squared_error(y_pred, y_valid))\n",
    "        val_scores.append(rmse)\n",
    "        \n",
    "    ## select best model\n",
    "    bm = regressor_names[np.argmin(val_scores)]\n",
    "    opt_model, params = models[bm]\n",
    "    \n",
    "    vocab = {\"RF\":\"Random Forest\",\n",
    "             \"SGD\":\"Stochastic Gradient\",\n",
    "             \"GB\":\"Gradient Boosting\",\n",
    "             \"ADA\":\"Ada Boosting\"}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"...best model:{}\".format(vocab[bm]))\n",
    "\n",
    "    ## retrain best model on the the full dataset\n",
    "    opt_model.fit(X, y)\n",
    "\n",
    "    ## Check the data directory\n",
    "    model_path=MODEL_DIR\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    if dev:\n",
    "        saved_model = os.path.join(model_path,\"test-{}-model-{}.joblib\".format(tag,re.sub(\"\\.\",\"_\",str(MODEL_VERSION))))\n",
    "    else:\n",
    "        saved_model = os.path.join(model_path,\"prod-{}-model-{}.joblib\".format(tag,re.sub(\"\\.\",\"_\",str(MODEL_VERSION))))\n",
    "\n",
    "    ## save the best model\n",
    "    joblib.dump(opt_model, saved_model)\n",
    "    \n",
    "    if save_img:\n",
    "        if 'feature_importances_' in dir(opt_model.best_estimator_[\"reg\"]):\n",
    "            _plot_feature_importance(opt_model.best_estimator_[\"reg\"], feature_names=feature_names, verbose=verbose)\n",
    "            save_fig(\"{}_features_importance\".format(tag))\n",
    "    \n",
    "    m, s = divmod(time.time()-time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "            \n",
    "    ## update log\n",
    "    _update_train_log(tag.upper(),vocab[bm],{'rmse':max(val_scores)},runtime,MODEL_VERSION, MODEL_VERSION_NOTE, dev=dev, verbose=verbose)\n",
    "\n",
    "\n",
    "def model_train(save_img=False,dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    train models\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load engineered features\n",
    "    datasets = engineer_features(dev=dev, training=True, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training Models\")\n",
    "    \n",
    "    ## build, train and save models\n",
    "    for country in datasets.keys():\n",
    "        tag = country\n",
    "        if verbose:\n",
    "            print(\"...training model for {}\".format(tag.upper()))\n",
    "        X, y, dates, feature_names = datasets[tag]\n",
    "        _model_train(X, y, feature_names, tag=tag, dev=dev, save_img=save_img, verbose=verbose)\n",
    "\n",
    "        \n",
    "def model_load(model_dir=MODEL_DIR, dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    load models\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading Models\")\n",
    "    \n",
    "    if dev:\n",
    "        prefix = \"test\"\n",
    "    else:\n",
    "        prefix = \"prod\"\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        raise Exception(\"Opps! Model dir does not exist\")\n",
    "    \n",
    "    ## list model files from model directory\n",
    "    models = [f for f in os.listdir(model_dir) if re.search(prefix,f)]\n",
    "\n",
    "    if len(models) == 0:\n",
    "        raise Exception(\"Models with prefix '{}' cannot be found did you train?\".format(prefix))\n",
    "    \n",
    "    ## load models\n",
    "    all_models = {}\n",
    "    for model in models:\n",
    "        all_models[re.split(\"-\",model)[1]] = joblib.load(os.path.join(model_dir,model))\n",
    "        \n",
    "    return(all_models)\n",
    "\n",
    "def model_predict(year, month, day, country, dev=DEV, verbose=True):\n",
    "    \"\"\"\n",
    "    make predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    ## start timer for runtime\n",
    "    time_start = time.time()\n",
    "    \n",
    "    ## load data\n",
    "    datasets = engineer_features(training=False, dev=dev, verbose=verbose)\n",
    "    \n",
    "    ## load models\n",
    "    models = model_load(dev=dev, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Make Prediction\")\n",
    "    \n",
    "    ## check if the model is available\n",
    "    if country not in models.keys():\n",
    "        raise Exception(\"ERROR (model_predict) - model for country '{}' could not be found\".format(country))\n",
    "    \n",
    "    ## ckeck if the data is available\n",
    "    if country not in datasets.keys():\n",
    "        raise Exception(\"ERROR (model_predict) - dataset for country '{}' could not be found\".format(country))\n",
    "    \n",
    "    ## ensure the year, month day are numbers\n",
    "    for d in [year,month,day]:\n",
    "        if re.search(\"\\D\",d):\n",
    "            raise Exception(\"ERROR (model_predict) - invalid year, month or day\")\n",
    "    \n",
    "    ## get the dataset and model for the given country    \n",
    "    X, y, dates, labels = datasets[country]\n",
    "    df = pd.DataFrame(X, columns=labels, index=dates)\n",
    "    model = models[country]\n",
    "    \n",
    "    ## check date\n",
    "    target_date = \"{}-{}-{}\".format(year,str(month).zfill(2),str(day).zfill(2))\n",
    "    \n",
    "    if verbose:\n",
    "        print(target_date)\n",
    "    \n",
    "    if target_date not in df.index.strftime('%Y-%m-%d'):\n",
    "        raise Exception(\"ERROR (model_predict) - {} not in range {} and {}\".format(target_date,df.index.strftime('%Y-%m-%d')[0],df.index.strftime('%Y-%m-%d')[-1]))\n",
    "    \n",
    "    ## query the data\n",
    "    query = pd.to_datetime(target_date)\n",
    "    X_pred = df.loc[pd.to_datetime(query),:].values.reshape(1, -1)\n",
    "    \n",
    "    ## make prediction\n",
    "    y_pred = model.predict(X_pred)\n",
    "    \n",
    "    m, s = divmod(time.time()-time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "    \n",
    "    ## update predict log\n",
    "    _update_predict_log(country.upper(),y_pred,target_date,runtime,MODEL_VERSION, MODEL_VERSION_NOTE,dev=dev, verbose=verbose)\n",
    "    \n",
    "    return({\"y_pred\":y_pred})\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    run_start = time.time()\n",
    "  \n",
    "    ## train models\n",
    "    model_train(dev=DEV)\n",
    "    \n",
    "    ## load models\n",
    "    models = model_load(dev=DEV, verbose=False)\n",
    "    \n",
    "    ## test predict\n",
    "    result = model_predict(country=\"total\",year=\"2019\",month=\"01\",day=\"05\", dev=DEV)\n",
    "        \n",
    "    ## metadata\n",
    "    print(\"METADATA\")\n",
    "    for key, item in models.items():\n",
    "        print(\"...label:{}, algorithm:{}\".format(key, type(item.best_estimator_[\"reg\"]).__name__))\n",
    "    \n",
    "    print(\"...result {}\".format(result))\n",
    "    \n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"...running time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "    \n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting Data\n",
      "...loading timeseries data from files\n",
      "Creating Feature Matrix\n",
      "Engineering Features and Target\n",
      "Training Models\n",
      "...training model for SINGAPORE\n",
      "...training model: 4/4\n",
      "...best model:Ada Boosting\n",
      "...updating train log\n",
      "...training model for NETHERLANDS\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for HONG_KONG\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for NORWAY\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for PORTUGAL\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for EIRE\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for TOTAL\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for GERMANY\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for SPAIN\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for FRANCE\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "...training model for UNITED_KINGDOM\n",
      "...training model: 4/4\n",
      "...best model:Stochastic Gradient\n",
      "...updating train log\n",
      "Ingesting Data\n",
      "...loading timeseries data from files\n",
      "Creating Feature Matrix\n",
      "Engineering Features and Target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eilhamv/.local/lib/python3.5/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/eilhamv/.local/lib/python3.5/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.23.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/eilhamv/.local/lib/python3.5/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.23.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/eilhamv/.local/lib/python3.5/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/eilhamv/.local/lib/python3.5/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.23.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models\n",
      "Make Prediction\n",
      "2019-01-05\n",
      "...update predict log\n",
      "METADATA\n",
      "...label:singapore, algorithm:AdaBoostRegressor\n",
      "...label:netherlands, algorithm:SGDRegressor\n",
      "...label:hong_kong, algorithm:SGDRegressor\n",
      "...label:norway, algorithm:SGDRegressor\n",
      "...label:portugal, algorithm:SGDRegressor\n",
      "...label:eire, algorithm:SGDRegressor\n",
      "...label:total, algorithm:SGDRegressor\n",
      "...label:germany, algorithm:SGDRegressor\n",
      "...label:spain, algorithm:SGDRegressor\n",
      "...label:france, algorithm:SGDRegressor\n",
      "...label:united_kingdom, algorithm:SGDRegressor\n",
      "...result {'y_pred': array([207974.85713583])}\n",
      "...running time: 0:01:29\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%run modelling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UNITED_KINGDOM</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GERMANY</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRANCE</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGAPORE</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HONG_KONG</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NORWAY</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPAIN</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETHERLANDS</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PORTUGAL</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EIRE</th>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model\n",
       "UNITED_KINGDOM       SGDRegressor\n",
       "GERMANY              SGDRegressor\n",
       "FRANCE               SGDRegressor\n",
       "SINGAPORE       AdaBoostRegressor\n",
       "HONG_KONG            SGDRegressor\n",
       "NORWAY               SGDRegressor\n",
       "SPAIN                SGDRegressor\n",
       "TOTAL                SGDRegressor\n",
       "NETHERLANDS          SGDRegressor\n",
       "PORTUGAL             SGDRegressor\n",
       "EIRE                 SGDRegressor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelling import model_load\n",
    "import pandas as pd\n",
    "\n",
    "models = model_load()\n",
    "\n",
    "labels = {}\n",
    "for key, item in models.items():\n",
    "    labels[key.upper()] = type(item.best_estimator_[\"reg\"]).__name__\n",
    "    \n",
    "bm = pd.DataFrame.from_dict(labels, orient=\"index\", columns=[\"model\"])\n",
    "bm"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
